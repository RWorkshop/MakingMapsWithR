
\documentclass[Chap1main.tex]{subfiles}
\begin{document}
	
	\section{Measurement Error Models}
	\textbf{DunnSEME} proposes a measurement error model for use in
	method comparison studies. Consider n pairs of measurements
	$X_{i}$ and $Y_{i}$ for $i=1,2,...n$.
	\begin{equation}
	X_{i} = \tau_{i}+\delta_{i}\\
	\end{equation}
	\begin{equation}
	Y_{i} = \alpha +\beta\tau_{i}+\epsilon_{i} \nonumber
	\end{equation}
	
	In the above formulation is in the form of a linear structural
	relationship, with $\tau_{i}$ and $\beta\tau_{i}$ as the true
	values , and $\delta_{i}$ and $\epsilon_{i}$ as the corresponding
	measurement errors. In the case where the units of measurement are
	the same, then $\beta =1$.
	
	\begin{equation}
	E(X_{i}) = \tau_{i}\\
	\end{equation}
	\begin{equation}
	E(Y_{i}) = \alpha +\beta\tau_{i} \nonumber
	\end{equation}
	\begin{equation}
	E(\delta_{i}) = E(\epsilon_{i}) = 0 \nonumber
	\end{equation}
	
	The value $\alpha$ is the inter-method bias between the two
	methods.
	
	\begin{eqnarray}
	z_0 &=& d = 0 \\
	z_{n+1} &=& z_n^2+c
	\end{eqnarray}
	
\section{Model Formulation and Formal Testing}

\citet{Kinsella} formulates a model for un-replicated observations
for a method comparison study as a mixed model.
\begin{eqnarray}
Y_{ij} =\quad \mu_{j} + S_{i} + \epsilon_{ij} \quad i=1,2...n\quad
j=1,2\\
S \sim N(0,\sigma^{2}_{s})\qquad \epsilon_{ij} \sim
N(0,\sigma^{2}_{j}) \nonumber
\end{eqnarray}

As with all mixed models, the variance of each observation is the
sum of all the associated variance components.
\begin{eqnarray}
var(Y_{ij}) =\quad \sigma^{2}_{s} + \sigma^{2}_{j} \\
cov(Y_{i1},Y_{i2})=\quad \sigma^{2}_{s} \nonumber
\end{eqnarray}

\citet{Grubbs48} offers maximum likelihood estimators, commonly
known as Grubbs estimators, for the various variance components:
\begin{eqnarray}
\hat{\sigma^{2}_{s}} \quad= \sum{\frac{(x_{i}-\bar{x})(y_{i}-\bar{y})}{n-1}}\quad=Sxy\\
\hat{\sigma^{2}_{1}} \quad= \sum{\frac{(x_{i}-\bar{x})^{2}}{n-1}} \quad=S^{2}x-Sxy \nonumber\\
\hat{\sigma^{2}_{2}} \quad=
\sum{\frac{(y_{i}-\bar{y})^{2}}{n-1}}\quad=S^{2}y-Sxy \nonumber
\nonumber
\end{eqnarray}

The standard error of these variance estimates are:
\begin{eqnarray}
var(\sigma^{2}_{1}) =\quad \frac{2\sigma^{4}_{1}}{n-1} +\quad
\frac{\sigma^2_{S}\sigma^2_{1}+\sigma^2_{S}\sigma^2_{2}+\sigma^2_{1}\sigma^2_{2}
}{n-1}\\
var(\sigma^{2}_{2}) =\quad \frac{2\sigma^{4}_{2}}{n-1} +\quad
\frac{\sigma^2_{S}\sigma^2_{1}+\sigma^2_{S}\sigma^2_{2}+\sigma^2_{1}\sigma^2_{2}
}{n-1}\nonumber
\end{eqnarray}

\citet{Thompson}presents confidence intervals for the relative
precisions of the measurement methods, $\Delta_{j}=
\sigma^2_{S}/\sigma^2_{j}$ (where $j=1,2$), as well as the
variances $\sigma^{2}_{S}, \sigma^{2}_{1}$ and $\sigma^{2}_{2}$.

\begin{eqnarray}
\Delta_{1} >\quad \frac{C_{xy}-
	t(|A|/n-2))^{\frac{1}{2}}}{C_{x}-C_{xy}+
	t(|A|/n-2))^{\frac{1}{2}}}
\end{eqnarray}
where

\begin{eqnarray}
C_{x}=\quad(n-1)S^2_{x}\nonumber\\
C_{xy}=\quad(n-1)S_{xy}\nonumber\\
C_{y}=\quad(n-1)S^2_{y}\nonumber\\
A=\quad C_{x}\times C_{y} - (C_{xy})^2 \nonumber
\end{eqnarray}

$t$ is the $100(1-\alpha/2)\%$ quantile of Student's $t$
distribution with $n-2$ degrees of freedom. $\Delta_{2}$ can be
found by changing $C_{y}$ for $C_{x}$. A lower confidence limit
can be found by calculating the square root. This inequality may
also be used for hypothesis testing.

For the interval estimates for the variance components,
\citet{Thompson} presents three relations that hold simultaneously
with probability $1-2\alpha$ where $2\alpha=0.01$ or $0.05$.

\begin{eqnarray}
|\sigma^2-C_{xy}K|\leqslant M(C_{x}C_{y})^{\frac{1}{2}}\\
|\sigma^2_{1}-(C_{x}-C_{xy})K|\leqslant M(C_{x}(C_{x}+C_{y}-2C_{xy}))^{\frac{1}{2}}\nonumber\\
|\sigma^2_{2}-(C_{y}-C_{xy})K|\leqslant
M(C_{y}(C_{x}+C_{y}-2C_{xy}))^{\frac{1}{2}}\nonumber
\end{eqnarray}

The case-wise differences and means are $D_{i} = Y_{i1}-Y_{i2}$
and $A_{i} = (Y_{i1}+Y_{i2})/2$  respectively. Both $D_{i}$ and
$A_{i}$ follow a bivariate normal distribution with $E(D_{i})=
\mu_{D} = \mu_{1} - \mu_{2}$ and $E(A_{i})= \mu_{A} = (\mu_{1} +
\mu_{2})/2$. The variance matrix $\Sigma$ is

\begin{equation}
\Sigma = \left[\begin{matrix}
\sigma^{2}_{1}+\sigma^{2}_{2}&\frac{1}{2}(\sigma^{2}_{1}-\sigma^{2}_{2})\\
\frac{1}{2}(\sigma^{2}_{1}-\sigma^{2}_{2})&\sigma^{2}_{S}+
\frac{1}{4}(\sigma^{2}_{1}+\sigma^{2}_{2})
\end{matrix} \right]
\end{equation}





\citet{Kinsella} demonstrates how the Grubbs estimators for the
error variances can be calculated using the difference values,
providing a worked example on a data set.
\begin{eqnarray}
\hat{\sigma^{2}_{1}}
\quad=\sum{(y_{i1}-\bar{y{1}})(D_{i}-\bar{D})}\\
\hat{\sigma^{2}_{2}} \quad=
\sum{(y_{i2}-\bar{y_{2}})(D_{i}-\bar{D})} \nonumber
\end{eqnarray}


\subsection{Morgan Pitman}

The test of the hypothesis that the variance of both methods are
equal is based on the correlation value $\rho_{D,A}$ which is
evaluated as follows;

\begin{equation}
\rho(D,A)=\quad\frac{\sigma^{2}_{1}-\sigma^{2}_{2}}{\sqrt{(\sigma^{2}_{1}+\sigma^{2}_{2})(4\sigma^{2}_{S}+\sigma^{2}_{1}+\sigma^{2}_{2})}}
\end{equation}

The correlation constant takes the value zero if, and only if, the
two variances are equal. Therefore a test of the hypothesis $H:
\sigma^{2}_{1}=\sigma^{2}_{2}$ is equivalent to a test of the
hypothesis $H: \rho(D,A) = 0$. The corresponds to the well-known
$t$ test for a correlation coefficient with $n-2$ degrees of
freedom.

\citet{Bartko} describes the Morgan-Pitman test as identical to
the test of the slope equal to zero in the regression of $Y_{i1}$
on $Y_{12}$, adding that this result can be shown using
straightforward algebra.

\subsection{Bartko's Bradley-Blackwood Test}
This is a regression based approach that performs a simulataneous
test for the equivalence of means and variances of the respective
methods.\\
\begin{equation}
D = (X_{1}-X_{2})
\end{equation}
\begin{equation}
M = (X_{1} + X_{2}) /2
\end{equation}
The Bradley Blackwood Procedure fits D on M as follows:\\
\begin{equation}
D = \beta_{0} + \beta_{1}M
\end{equation}
\\Both beta values, the intercept and slope, are derived from the respective means and
standard deviations of their respective data sets.\\
We determine if the respective means and variances are equal if
both beta values are simultaneously equal to zero. The Test is
conducted using an F test, calculated from the results of a
regression of D on M.
\\We have identified this approach  to be examined to see if it can
be used as a foundation for a test perform a test on means and
variances individually.\\
Russell et al have suggested this method be used in conjunction
with a paired t-test , with estimates of slope and intercept.

subsection{t-test}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%  Blackwood Bradley Model         %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Blackwood Bradley Model} This is a regression based
approach that performs a simultaneous test for the equivalence of
means and variances of the respective methods.\\We have identified
this approach  to be examined to see if it can be used as a
foundation for a test perform a test on
means and variances individually.\\
\begin{equation}
D = (X_{1}-X_{2})
\end{equation}
\begin{equation}
M = (X_{1} + X_{2}) /2
\end{equation}
The Bradley Blackwood Procedure fits D on M as follows:\\
\begin{equation}
D = \beta_{0} + \beta_{1}M
\end{equation}
\\Both beta values, the intercept and slope, are derived from the respective means and
standard deviations of their respective data sets.\\
We determine if the respective means and variances are equal if
both beta values are simultaneously equal to zero. The Test is
conducted using an F test, calculated from the results of a
regression of D on M.
\\
Russell et al have suggested this method be used in conjunction
with a paired t-test , with estimates of slope and intercept.
Bradley and Blackwood have developed a regression based approach
assessing the agreement.
\\
The Bradley Blackwood test is a simultaneous test for bias and
precision. They propose a regression approach which fits D on M,
where D is the difference and average of a pair of results.



\subsection{Pitman \& Morgan Test} This test assess the equality
of population variances. Pitman's test tests for zero correlation
between the sums and products.
\\
Correlation between differences and means is a test statistics for
the null hypothesis of equal variances given bivariate normality.
\section{Thompson 1963}



\citet{Thompson} defines $\Delta_{j}$ to be a measure of the
relative precision of the measurement methods, with $\Delta_{j}=
\sigma^2_{S}/\sigma^2_{j}$(where $j=1,2$). Confidence intervals
for $\Delta_{j}$ are also presented.

\begin{eqnarray}
\Delta_{1} > \frac{C_{xy}-
	t(\frac{|A|}{n-1}))^{\frac{1}{2}}}{C_{x}-C_{xy}+
	t(\frac{|A|}{n-1}))^{\frac{1}{2}}},
\end{eqnarray}
where

\begin{eqnarray}
C_{x}&=&(n-1)S^2_{x},\nonumber\\
C_{xy}&=&(n-1)S_{xy},\nonumber\\
C_{y}&=&(n-1)S^2_{y},\nonumber\\
A &=& C_{x}\times C_{y} - (C_{xy})^2 . \nonumber
\end{eqnarray}

The value $t$ is the $100(1-\alpha/2)\%$ quantile of Student's $t$
distribution with $n-2$ degrees of freedom. The ratio $\Delta_{2}$
can be found by interchanging $C_{y}$ and $C_{x}$. A lower
confidence limit can be found by calculating the square root. The
inequality in equation $1.10$ may also be used for hypothesis
testing.

For the interval estimates for the variance components,
\citet{Thompson} presents three relations that hold simultaneously
with probability $1-2\alpha$ where $2\alpha=0.01$ or $0.05$.


\begin{eqnarray*}
	|\sigma^2-C_{xy}K| &\leqslant& M(C_{x}C_{y})^{\frac{1}{2}}\\
	|\sigma^2_{1}-(C_{x}-C_{xy})K|&\leqslant M(C_{x}(C_{x}+C_{y}-2C_{xy}))^{\frac{1}{2}}\nonumber\\
	|\sigma^2_{2}-(C_{y}-C_{xy})K|&\leqslant
	M(C_{y}(C_{x}+C_{y}-2C_{xy}))^{\frac{1}{2}}\nonumber
\end{eqnarray*}

\citet{Thompson} contains tables for $K$ and $M$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Bartko's BB
\citet{BB89} offers a formal simultaneous hypothesis test for the
mean and variance of two paired data sets. Using simple linear
regression of the differences of each pair against the sums, a
line is fitted to the model, with estimates for intercept and
slope ($\hat{\beta}_{0}$ and $\hat{\beta}_{1}$). The null
hypothesis of this test is that the mean ($\mu$) and variance
($\sigma^{2}$) of both data sets are equal if the slope and
intercept estimates are equal to zero(i.e $\sigma^{2}_{1} =
\sigma^{2}_{2}$ and $\mu_{1}=\mu_{2}$ if and only if $\beta_{0}=
\beta_{1}=0$ )

A test statistic is then calculated from the regression analysis
of variance values \citep{BB89} and is distributed as `F' random
variable. The degrees of freedom thereof are $\nu_{1}=2$ and
$\nu_{1}=n-2$ (where n is the number of pairs). The critical value
is chosen for $\alpha\%$ significance with those same degrees of
freedom. \citet{Bartko} amends this methodology for use in method
comparison studies, using the averages of the pairs, as opposed to
the sums, and their differences. This approach can facilitate
simultaneous usage of test with the Bland-Altman methodology.
Bartko's test statistic take the form:
\begin{equation} F.test = \frac{(\Sigma d^{2})-SSReg}{2MSReg}
\end{equation}
% latex table generated in R 2.6.0 by xtable 1.5-5 package
% Mon Aug 31 15:53:51 2009
\begin{table}[ht]
	\begin{center}
		\begin{tabular}{lrrrrr}
			\hline
			& Df & Sum Sq & Mean Sq & F value & Pr($>$F) \\
			\hline
			Averages & 1 & 0.04 & 0.04 & 0.74 & 0.4097 \\
			Residuals & 10 & 0.60 & 0.06 &  &  \\
			\hline
		\end{tabular}
		\caption{Regression ANOVA of case-wise differences and averages
			for Grubbs Data}
	\end{center}
\end{table}
%(calculate using R code $qf(0.95,2,10)$).

For the Grubbs data, $\Sigma d^{2}=5.09 $, $SSReg = 0.60$ and
$MSreg=0.06$ Therefore the test statistic is $37.42$, with a
critical value of $4.10$. Hence the means and variance of the
Fotobalk and Counter chronometers are assumed to be simultaneously
equal.

Importantly, this methodology determines whether there is both
inter-method bias and precision present, or alternatively if there
is neither present. It has previously been demonstrated that there
is a inter-method bias present, but as this procedure does not
allow for separate testing, no conclusion can be drawn on the
comparative precision of both methods.

\subsection{Formal Testing}
The Bland Altman plot is a simple tool for inspection of the data,
but in itself it offers no formal testing procedure in this
regard. To this end, the approach proposed by \citet{BA83} is a
formal test on the Pearson correlation coefficient  of casewise
differences and means ($\rho_{AD}$). According to the authors,
this test is equivalent to a well established tests for equality
of variances, known as the `Pitman Morgan Test' \citep{Pitman,
	Morgan}.

For the Grubbs data, the correlation coefficient estimate
($r_{AD}$) is 0.2625, with a 95\% confidence interval of (-0.366,
0.726) estimated by Fishers 'r to z' transformation \citep{Cohen}.
The null hypothesis ($\rho_{AD}$ =0) would fail to be rejected.
Consequently the null hypothesis of equal variances of each method
would also fail to be rejected.

There has no been no further mention of this particular test in
the subsequent article published by Bland and Altman, although
\citet{BA99} refers to Spearmans' rank correlation coefficient.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\section{Bartko's Regression and Ellipse}
\citet{BB89} offers a formal simultaneous hypothesis test for the
mean and variance of two paired data sets. Using simple linear
regression of the differences of each pair against the sums, a
line is fitted to the model, with estimates for intercept and
slope ($\beta_{0}$ and $\beta_{1}$). The null hypothesis of this
test is that the mean ($\mu$) and variance ($\sigma^{2}$) of both
data sets are equal if the slope and intercept estimates are equal
to zero(i.e $\sigma^{2}_{1} = \sigma^{2}_{2}$ and
$\mu_{1}=\mu_{2}$ if and only if $\beta_{0}= \beta_{1}=0$ )

A test statistic is then calculated from the regression analysis
of variance values \citep{BB89} and is distributed as `F' random
variable. The degrees of freedom thereof are $\nu_{1}=2$ and
$\nu_{1}=n-2$ (where n is the number of pairs). The critical value
is chosen for $\alpha\%$ significance with those same degrees of
freedom. \citet{Bartko} amends this metholodogy for calculation
using the from the averages of the pairs, as opposed to the sums,
and their differences. This would facilitate simultaneous usage of
test with the Bland Altman methodology. Bartko's test statistic
take the form:
\begin{equation} F.test = \frac{(\Sigma D^{2})-SSReg}{2MSReg}
\end{equation}

\newpage

% latex table generated in R 2.6.0 by xtable 1.5-5 package
% Mon Aug 31 15:53:51 2009
\begin{table}[ht]
	\begin{center}
		\begin{tabular}{lrrrrr}
			\hline
			& Df & Sum Sq & Mean Sq & F value & Pr($>$F) \\
			\hline
			Averages & 1 & 0.04 & 0.04 & 0.74 & 0.4097 \\
			Residuals & 10 & 0.60 & 0.06 &  &  \\
			\hline
		\end{tabular}
		\caption{Regression ANOVA of case-wise differences and averages
			for Grubbs Data}
	\end{center}
\end{table}




For the Grubbs data, $\Sigma D^{2}=5.09 $, $SSReg = 0.60$ and
$MSreg=0.06$ Therefore the test statistic is $37.42$, with a
critical value of $4.102821$ (calculate using r code
$qf(0.95,2,10)$). Hence the means and variance of the Fotobalk and
Counter chronometers are assumed to be simultaneously equal.

Importantly, this methodology determines whether there is both
inter-method bias and precision present, or alternatively if there
is neither present. It has previously been demonstrated that there
is a inter-method bias present, but as this procedure does not
allow for seperate testing, no conclusion can be drawn on the
comparative precision of both methods.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Formal Models and Tests}
The Bland-Altman plot is a simple tool for inspection of data, and
\citet{Kinsella} comments on the lack of formal testing offered by
that methodology. \citet{Kinsella} formulates a model for
single measurement observations for a method comparison study as a
linear mixed effects model, i.e. model that additively combine
fixed effects and random effects.
\[
Y_{ij} =\quad \mu + \beta_{j} + u_{i} + \epsilon_{ij} \qquad i = 1,\dots,n
\qquad j=1,2\]

The true value of the measurement is represented by $\mu$ while the fixed effect due to method $j$ is $\beta_{j}$.
For simplicity these terms can be combined into single terms; $\mu_{1} = \mu+ \beta_{1}$ and $\mu_{2} = \mu + \beta_{2}$. The inter-method bias is the difference of the two fixed effect terms, $\beta_{1}-\beta_{2}$. Each of the $i$ individuals are assumed to give rise to random error, represented by $u_{i}$. This random effects terms is assumed to have mean zero and be normally distributed with variance $\sigma^2$. There is assumed to be an attendant error for each measurement on each individual, denoted $\epsilon_{ij}$. This is also assumed to have mean zero. The variance of measurement error for both methods are not assumed to be identical for both methods variance,  hence it is denoted $\sigma^2_{j}$. The set of observations ($x_{i},y_{i}$) by methods $X$ and $Y$ are assumed to follow the bivariate normal distribution with expected values $E(x_{i})= \mu_{i}$ and $E(x_{i})= \mu_{i}$ respectively. The variance covariance of the observations $\boldsymbol{\Sigma}$ is given by

\[
\boldsymbol{\Sigma} = \left[
                        \begin{array}{cc}
                          \sigma^{2} + \sigma^{2}_{1} & \sigma^{2} \\
                          \sigma^{2} & \sigma^{2} + \sigma^{2}_{2} \\
                        \end{array}
                      \right]
\]

The inter-method bias is the difference of the two fixed effect terms, $\beta_{1}-\beta_{2}$.

\citet{Kinsella} demonstrates the estimation of the variance terms and relative precisions relevant to a method comparison study, with attendant confidence intervals for both. The measurement model introduced by \citet{Grubbs48,Grubbs73} provides a formal procedure for estimate the variances $\sigma^2$,$\sigma^2_{1}$ and $\sigma^2_{2}$ devices. \citet{Grubbs48} offers estimates, commonly known as Grubbs estimators, for the various variance components. These estimates are maximum likelihood estimates, a statistical concept that shall be revisited in due course.
\begin{eqnarray*}
\hat{\sigma^{2}} = \sum{\frac{(x_{i}-\bar{x})(y_{i}-\bar{y})}{n-1}} = Sxy\\
\hat{\sigma^{2}_{1}} = \sum{\frac{(x_{i}-\bar{x})^{2}}{n-1}} =S^{2}x - Sxy  \\
\hat{\sigma^{2}_{2}} =
\sum{\frac{(y_{i}-\bar{y})^{2}}{n-1}} = S^{2}y - Sxy
\end{eqnarray*}

% The standard error of these variance estimates are:
% \begin{eqnarray}
% \mbox{var}(\sigma^{2}_{1}) = \frac{2\sigma^{4}_{1}}{n-1} +
% \frac{\sigma^2_{S}\sigma^2_{1}+\sigma^2_{S}\sigma^2_{2}+\sigma^2_{1}\sigma^2_{2}
% }{n-1}\\
% \mbox{var}(\sigma^{2}_{2}) =\quad \frac{2\sigma^{4}_{2}}{n-1} +
% \frac{\sigma^2_{S}\sigma^2_{1}+\sigma^2_{S}\sigma^2_{2}+\sigma^2_{1}\sigma^2_{2}
% }{n-1}\nonumber
% \end{eqnarray}

\citet{Thompson} defines $\Delta_{j}$ to be a measure of the
relative precision of the measurement methods, with $\Delta_{j}=
\sigma^2/\sigma^2_{j}$. Thompson also demonstrates how to make statistical inferences about $\Delta_{j}$.
Based on the following identities,
\begin{eqnarray*}
C_{x}&=&(n-1)S^2_{x},\nonumber\\
C_{xy}&=&(n-1)S_{xy},\nonumber\\
C_{y}&=&(n-1)S^2_{y},\nonumber\\
|A| &=& C_{x}\times C_{y} - (C_{xy})^2,\nonumber
\end{eqnarray*}
\noindent the confidence interval limits of $\Delta_{1}$ are

\begin{eqnarray}
\Delta_{1} > \frac{C_{xy}-
t(\frac{|A|}{n-2}))^{\frac{1}{2}}}{C_{x}-C_{xy}+
t(\frac{|A|}{n-2}))^{\frac{1}{2}}} \\
\Delta_{1} > \frac{C_{xy}+
t(\frac{|A|}{n-2}))^{\frac{1}{2}}}{C_{x}-C_{xy}-
t(\frac{|A|}{n-1}))^{\frac{1}{2}}} \nonumber
\end{eqnarray}
\\ The value $t$ is the $100(1-\alpha/2)\%$ upper quantile of
Student's $t$ distribution with $n-2$ degrees of freedom
\citep{Kinsella}. The confidence limits for $\Delta_{2}$ are found by substituting $C_{y}$ for $C_{x}$ in (1.3).
Negative lower limits are replaced by the value $0$.

%For the interval estimates for the variance components,
%\citet{Thompson} presents three relations that hold simultaneously
%with probability $1-2\alpha$ where $2\alpha=0.01$ or $0.05$.

%\begin{eqnarray*}
%|\sigma^2-C_{xy}K| &\leqslant& M(C_{x}C_{y})^{\frac{1}{2}}\\
%|\sigma^2_{1}-(C_{x}-C_{xy})K|&\leqslant M(C_{x}(C_{x}+C_{y}-2C_{xy}))^{\frac{1}{2}}\nonumber\\
%|\sigma^2_{2}-(C_{y}-C_{xy})K|&\leqslant
%M(C_{y}(C_{x}+C_{y}-2C_{xy}))^{\frac{1}{2}}\nonumber
%\end{eqnarray*}

%\citet{Thompson} contains tables for $K$ and $M$.

The case-wise differences and means are calculated as $d_{i} =
x_{i}-y_{i}$ and $a_{i} = (x_{i}+y_{i})/2$  respectively. Both
$d_{i}$ and $a_{i}$ are assumed to follow a bivariate normal
distribution with $E(d_{i})= \mu_{d} = \mu_{1} - \mu_{2}$ and
$E(a_{i})= \mu_{a} = (\mu_{1} + \mu_{2})/2$. The variance matrix
$\Sigma_{(a,d)}$ is

\begin{eqnarray}
\Sigma_{(a,d)}= \left[\begin{matrix}
\sigma^{2}_{1}+\sigma^{2}_{2}&\frac{1}{2}(\sigma^{2}_{1}-\sigma^{2}_{2})\\
\frac{1}{2}(\sigma^{2}_{1}-\sigma^{2}_{2})&\sigma^{2}+
\frac{1}{4}(\sigma^{2}_{1}+\sigma^{2}_{2})
\end{matrix} \right].
\end{eqnarray}



\subsection{Morgan-Pitman Testing}
An early contribution to formal testing in method comparison was
made by both \citet{morgan} and \citet{pitman}, in separate
contributions. The basis of this approach is that if the
distribution of the original measurements is bivariate normal.
Morgan and Pitman noted that the correlation coefficient depends
upon the difference $\sigma^{2}_{1}- \sigma^{2}_{2}$, being zero
if and only if $\sigma^{2}_{1}=\sigma^{2}_{2}$.

The classical Pitman-Morgan test is a hypothesis test for equality
of the variance of two data sets; $\sigma^{2}_{1} =
\sigma^{2}_{2}$, based on the correlation value $\rho_{a,d}$ ,and
is evaluated as follows;

\begin{equation}
\rho(a,d)=\quad\frac{\sigma^{2}_{1}-\sigma^{2}_{2}}{\sqrt{(\sigma^{2}_{1}+\sigma^{2}_{2})(4\sigma^{2}_{S}+\sigma^{2}_{1}+\sigma^{2}_{2})}}
\end{equation}

The correlation constant takes the value zero if, and only if, the two variances are equal. Therefore a test of the hypothesis $H: \sigma^{2}_{1}=\sigma^{2}_{2}$ is equivalent to a test of the hypothesis $H: \rho(D,A) = 0$. The corresponds to the well-known
$t$ test for a correlation coefficient with $n-2$ degrees of freedom. \citet{Bartko} describes the Morgan-Pitman test as identical to
the test of the slope equal to zero in the regression of $Y_{i1}$ on $Y_{12}$, a result that can be derived using
straightforward algebra.



\end{document}